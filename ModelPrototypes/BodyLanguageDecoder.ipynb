{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A Classification Model Designed for Multiclass Pose Detection with Scklearn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install and Import Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "!pip install mediapipe opencv-python pandas scikit-learn"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (0.8.6)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (4.5.3.56)\n",
      "Requirement already satisfied: pandas in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from mediapipe) (3.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from mediapipe) (1.21.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from mediapipe) (0.35.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from mediapipe) (4.5.3.56)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from mediapipe) (20.3.0)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from mediapipe) (3.17.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from mediapipe) (0.13.0)\n",
      "Requirement already satisfied: six in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from mediapipe) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (8.3.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\faraz khoubsirat\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.4.7)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Custom Model Using Scikit Learn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preproccesing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# import dependencies\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import mediapipe as mp \r\n",
    "import cv2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "# Make a dataframe\r\n",
    "df = pd.read_csv('coords.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "df.groupby(df['class']).count()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        x1   y1   z1   v1   x2   y2   z2   v2   x3   y3  ...  z499  v499  \\\n",
       "class                                                    ...               \n",
       "Tried  129  129  129  129  129  129  129  129  129  129  ...   129   129   \n",
       "angry  323  323  323  323  323  323  323  323  323  323  ...   323   323   \n",
       "happy  155  155  155  155  155  155  155  155  155  155  ...   155   155   \n",
       "\n",
       "       x500  y500  z500  v500  x501  y501  z501  v501  \n",
       "class                                                  \n",
       "Tried   129   129   129   129   129   129   129   129  \n",
       "angry   323   323   323   323   323   323   323   323  \n",
       "happy   155   155   155   155   155   155   155   155  \n",
       "\n",
       "[3 rows x 2004 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>v1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>z2</th>\n",
       "      <th>v2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>...</th>\n",
       "      <th>z499</th>\n",
       "      <th>v499</th>\n",
       "      <th>x500</th>\n",
       "      <th>y500</th>\n",
       "      <th>z500</th>\n",
       "      <th>v500</th>\n",
       "      <th>x501</th>\n",
       "      <th>y501</th>\n",
       "      <th>z501</th>\n",
       "      <th>v501</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tried</th>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angry</th>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>...</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>...</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 2004 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(607, 2005)"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "X = df.drop('class', axis=1) # features\r\n",
    "y = df['class'] # target value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# Split the dataset\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1234)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "from sklearn.pipeline import make_pipeline\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "pipelines = {\r\n",
    "    'lr':make_pipeline(StandardScaler(), LogisticRegression()),\r\n",
    "    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\r\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\r\n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\r\n",
    "\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "fit_models = {}\r\n",
    "for algo, pipeline in pipelines.items():\r\n",
    "    model = pipeline.fit(X_train, y_train)\r\n",
    "    fit_models[algo] = model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "fit_models"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'lr': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('logisticregression', LogisticRegression())]),\n",
       " 'rc': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('ridgeclassifier', RidgeClassifier())]),\n",
       " 'rf': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('randomforestclassifier', RandomForestClassifier())]),\n",
       " 'gb': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('gradientboostingclassifier', GradientBoostingClassifier())])}"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation and Serialization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# Get Model Accuracies\r\n",
    "model_accuracy = {}\r\n",
    "for algo, model in fit_models.items():\r\n",
    "    yhat = model.predict(X_test)\r\n",
    "    model_accuracy[algo] = accuracy_score(y_test, yhat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "model_accuracy"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'lr': 1.0, 'rc': 1.0, 'rf': 1.0, 'gb': 1.0}"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "# Choose the best model\r\n",
    "best_model = max(model_accuracy)\r\n",
    "best_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'rf'"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "# Save the model\r\n",
    "with open('body_language_decoder.pkl', 'wb') as f:\r\n",
    "    pickle.dump(fit_models[best_model], f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Detections with the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "with open('body_language_decoder.pkl', 'rb') as f:\r\n",
    "    model = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helper\r\n",
    "mp_holistic = mp.solutions.holistic\r\n",
    "\r\n",
    "cap = cv2.VideoCapture(0)\r\n",
    "# Initiate holistic model\r\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\r\n",
    "    \r\n",
    "    while cap.isOpened():\r\n",
    "        ret, frame = cap.read()\r\n",
    "        \r\n",
    "        # Recolor Feed\r\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n",
    "        image.flags.writeable = False        \r\n",
    "        \r\n",
    "        # Make Detections\r\n",
    "        results = holistic.process(image)\r\n",
    "        # print(results.face_landmarks)\r\n",
    "        \r\n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\r\n",
    "        \r\n",
    "        # Recolor image back to BGR for rendering\r\n",
    "        image.flags.writeable = True   \r\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "        \r\n",
    "        # 1. Draw face landmarks\r\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \r\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\r\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\r\n",
    "                                 )\r\n",
    "        \r\n",
    "        # 2. Right hand\r\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \r\n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\r\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\r\n",
    "                                 )\r\n",
    "\r\n",
    "        # 3. Left Hand\r\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \r\n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\r\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\r\n",
    "                                 )\r\n",
    "\r\n",
    "        # 4. Pose Detections\r\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \r\n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\r\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\r\n",
    "                                 )\r\n",
    "        # Export coordinates\r\n",
    "        try:\r\n",
    "            # Extract Pose landmarks\r\n",
    "            pose = results.pose_landmarks.landmark\r\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\r\n",
    "            \r\n",
    "            # Extract Face landmarks\r\n",
    "            face = results.face_landmarks.landmark\r\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\r\n",
    "            \r\n",
    "            # Concate rows\r\n",
    "            row = pose_row+face_row\r\n",
    "            \r\n",
    "#             # Append class name \r\n",
    "#             row.insert(0, class_name)\r\n",
    "            \r\n",
    "#             # Export to CSV\r\n",
    "#             with open('coords.csv', mode='a', newline='') as f:\r\n",
    "#                 csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\r\n",
    "#                 csv_writer.writerow(row) \r\n",
    "\r\n",
    "            # Make Detections\r\n",
    "            X = pd.DataFrame([row])\r\n",
    "            body_language_class = model.predict(X)[0]\r\n",
    "            body_language_prob = model.predict_proba(X)[0]\r\n",
    "            #print(body_language_class, body_language_prob)\r\n",
    "            \r\n",
    "            # Grab ear coords\r\n",
    "            coords = tuple(np.multiply(\r\n",
    "                            np.array(\r\n",
    "                                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \r\n",
    "                                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\r\n",
    "                        , [640,480]).astype(int))\r\n",
    "            \r\n",
    "            cv2.rectangle(image, \r\n",
    "                          (coords[0], coords[1]+5), \r\n",
    "                          (coords[0]+len(body_language_class)*20, coords[1]-30), \r\n",
    "                          (245, 117, 16), -1)\r\n",
    "            cv2.putText(image, body_language_class, coords, \r\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
    "            \r\n",
    "            # Get status box\r\n",
    "            cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\r\n",
    "            \r\n",
    "            # Display Class\r\n",
    "            cv2.putText(image, 'CLASS'\r\n",
    "                        , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\r\n",
    "            cv2.putText(image, body_language_class.split(' ')[0]\r\n",
    "                        , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
    "            \r\n",
    "            # Display Probability\r\n",
    "            cv2.putText(image, 'PROB'\r\n",
    "                        , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\r\n",
    "            cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\r\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
    "            \r\n",
    "        except:\r\n",
    "            pass\r\n",
    "                        \r\n",
    "        cv2.imshow('Raw Webcam Feed', image)\r\n",
    "\r\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\r\n",
    "            break\r\n",
    "\r\n",
    "cap.release()\r\n",
    "cv2.destroyAllWindows()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# import traceback\r\n",
    "\r\n",
    "# mp_drawing = mp.solutions.drawing_utils # Drawing helper\r\n",
    "# mp_hands = mp.solutions.hands\r\n",
    "\r\n",
    "# cap = cv2.VideoCapture(0)\r\n",
    "# # Initiate holistic model\r\n",
    "# with mp_hands.Hands(max_num_hands=1) as hand:\r\n",
    "\r\n",
    "#     while cap.isOpened():\r\n",
    "#         ret, frame = cap.read()\r\n",
    "\r\n",
    "#         # Recolor the image\r\n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n",
    "#         image.flags.writeable = False\r\n",
    "#         # Make Detections\r\n",
    "#         results = hand.process(image)\r\n",
    "\r\n",
    "#         # Landmark detection\r\n",
    "#         image.flags.writeable = True\r\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "        \r\n",
    "#         if results.multi_hand_landmarks:\r\n",
    "#             for handLms in results.multi_hand_landmarks:\r\n",
    "#                     mp_drawing.draw_landmarks(image, handLms, mp_hands.HAND_CONNECTIONS)\r\n",
    "\r\n",
    "#         if results.multi_hand_landmarks:\r\n",
    "#             # Export coordinates\r\n",
    "#             try:\r\n",
    "#                 # Extracting hand landmarks\r\n",
    "#                 # Extracting pose landmark\r\n",
    "#                 detected_hand = results.multi_hand_landmarks[0].landmark\r\n",
    "#                 hand_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in detected_hand]).flatten())\r\n",
    "#                 #print(list([landmark for landmark in detected_hand]))\r\n",
    "\r\n",
    "#                 # Concatanate rows\r\n",
    "#                 row = hand_row\r\n",
    "\r\n",
    "#                 # Make Detections\r\n",
    "#                 X = pd.DataFrame([row])\r\n",
    "#                 body_language_class = model.predict(X)[0]\r\n",
    "#                 body_language_prob = model.predict_proba(X)[0]\r\n",
    "#                 #print(body_language_class, body_language_prob)\r\n",
    "                \r\n",
    "#                 # Get status box\r\n",
    "#                 cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\r\n",
    "                \r\n",
    "#                 # Display Class\r\n",
    "#                 cv2.putText(image, 'CLASS'\r\n",
    "#                             , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\r\n",
    "#                 cv2.putText(image, body_language_class.split(' ')[0]\r\n",
    "#                             , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
    "                \r\n",
    "#                 # Display Probability\r\n",
    "#                 cv2.putText(image, 'PROB'\r\n",
    "#                             , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\r\n",
    "#                 cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\r\n",
    "#                             , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
    "                \r\n",
    "#             except:\r\n",
    "#                 traceback.print_exc()\r\n",
    "                            \r\n",
    "#         cv2.imshow('Raw Webcam Feed', image)\r\n",
    "\r\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\r\n",
    "#             break\r\n",
    "\r\n",
    "# cap.release()\r\n",
    "# cv2.destroyAllWindows()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "1df6b99b5e9756eed4e22fb0d124bf8d9f8552b372ae034d7c6b29f98bd0c98b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}